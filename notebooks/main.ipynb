{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch import optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from bank_telemarketing.config import config\n",
    "from bank_telemarketing.features.build_features import CustomDataset\n",
    "from bank_telemarketing.models.models import Classifier\n",
    "from bank_telemarketing.preprocessing.embedding import CategoricalEmbeddingSizes\n",
    "from bank_telemarketing.preprocessing.preprocess import CustomScaler\n",
    "from bank_telemarketing.preprocessing.preprocess import MultiLabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I - Data preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I.1 - Import data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/clean/clean_bank_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I.2 define columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "x_emb_cols = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"month\", \"day_of_week\", \"poutcome\"]\n",
    "\n",
    "x_bin_col = [\"contact\", \"has_been_contacted\"]\n",
    "\n",
    "y_col = \"subscribed\"\n",
    "\n",
    "x_numerical = [\"age\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\n",
    "               \"euribor3m\", \"nr.employed\"]\n",
    "\n",
    "cats = x_emb_cols + x_bin_col\n",
    "all_cols = x_emb_cols + x_bin_col + x_numerical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# set columns categories types\n",
    "data[cats] = data[cats].astype(\"category\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I-3 Apply preprocessing pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_pipe = Pipeline(\n",
    "    steps=\n",
    "    [\n",
    "        (\"label_encoder\", MultiLabelEncoder(cols=cats)),\n",
    "        (\"scaler\", CustomScaler(cols=x_numerical))\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_mapping = {\"no\": 0, \"yes\": 1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "y = data.subscribed\n",
    "X = data.drop([\"subscribed\", \"duration\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I.4 - Create train and test batch data (tensor) for the training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Train data\n",
    "y_train = y_train.replace(y_mapping).reset_index(drop=True)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_train = x_pipe.fit_transform(X_train)\n",
    "train_dataset = CustomDataset(\n",
    "    emb_cols=x_emb_cols,\n",
    "    x=X_train,\n",
    "    y=y_train\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=config[\"TRAIN_BS\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Validation data\n",
    "y_test = y_test.replace(y_mapping).reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "X_test = x_pipe.transform(X_test)\n",
    "test_dataset = CustomDataset(\n",
    "    emb_cols=x_emb_cols,\n",
    "    x=X_test,\n",
    "    y=y_test\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=config[\"VALID_BS\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I.5 - Get categorical embedding parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Get embedding size\n",
    "emb = CategoricalEmbeddingSizes().get_cat_emb_dims(data=X_train, cat_cols=cats)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II - Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### II.1 - Instantiate model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Classifier(\n  (dropout): Dropout(p=0.3, inplace=False)\n  (bn): BatchNorm1d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (embedding_layers): ModuleList(\n    (0): Embedding(13, 7)\n    (1): Embedding(5, 3)\n    (2): Embedding(9, 5)\n    (3): Embedding(4, 2)\n    (4): Embedding(4, 2)\n    (5): Embedding(4, 2)\n    (6): Embedding(11, 6)\n    (7): Embedding(6, 3)\n    (8): Embedding(4, 2)\n  )\n  (linear): Sequential(\n    (0): Linear(in_features=43, out_features=64, bias=True)\n    (1): ReLU()\n    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): ReLU()\n    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Linear(in_features=32, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(\n",
    "    hidden=64,\n",
    "    output_size=1,\n",
    "    continuous_size=11,\n",
    "    embedding_sizes=emb,\n",
    "    dropout=0.3\n",
    ")\n",
    "model.to(config[\"DEVICE\"])\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "BCEWithLogitsLoss()"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config[\"LR\"])\n",
    "cyclic_optimizer = optim.SGD(model.parameters(), lr=config[\"LR\"], momentum=0.9)\n",
    "\n",
    "pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "criterion.to(config[\"DEVICE\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "linear_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    \"min\",\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "cyclic_scheduler = lr_scheduler.CyclicLR(\n",
    "    optimizer=cyclic_optimizer,\n",
    "    base_lr=config[\"LR\"],\n",
    "    max_lr=0.1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Training Started ============================\n",
      "Epoch: 1    | Elapsed Time:  10.24 s | Train Loss:  0.0435 | Valid Loss:  0.0031 | Train F1:  0.4495 | Valid F1:  0.4238 | Train Precision:  0.3449 | Valid Precision:  0.2990 |  + \n",
      "Epoch: 2    | Elapsed Time:  12.63 s | Train Loss:  0.0436 | Valid Loss:  0.0031 | Train F1:  0.4502 | Valid F1:  0.4676 | Train Precision:  0.3440 | Valid Precision:  0.3579 |  + \n",
      "Epoch: 3    | Elapsed Time:  14.34 s | Train Loss:  0.0433 | Valid Loss:  0.0031 | Train F1:  0.4514 | Valid F1:  0.4757 | Train Precision:  0.3485 | Valid Precision:  0.3690 |  + \n",
      "Epoch: 4    | Elapsed Time:  14.16 s | Train Loss:  0.0434 | Valid Loss:  0.0031 | Train F1:  0.4527 | Valid F1:  0.4667 | Train Precision:  0.3470 | Valid Precision:  0.3551 | \n",
      "Epoch: 5    | Elapsed Time:  12.23 s | Train Loss:  0.0435 | Valid Loss:  0.0031 | Train F1:  0.4490 | Valid F1:  0.4787 | Train Precision:  0.3438 | Valid Precision:  0.3736 |  + \n",
      "Epoch: 6    | Elapsed Time:  16.44 s | Train Loss:  0.0436 | Valid Loss:  0.0031 | Train F1:  0.4510 | Valid F1:  0.4773 | Train Precision:  0.3452 | Valid Precision:  0.3743 |  + \n",
      "Epoch: 7    | Elapsed Time:  13.78 s | Train Loss:  0.0432 | Valid Loss:  0.0031 | Train F1:  0.4514 | Valid F1:  0.4777 | Train Precision:  0.3478 | Valid Precision:  0.3708 | \n",
      "Epoch: 8    | Elapsed Time:  16.04 s | Train Loss:  0.0433 | Valid Loss:  0.0031 | Train F1:  0.4496 | Valid F1:  0.4883 | Train Precision:  0.3447 | Valid Precision:  0.3899 |  + \n",
      "Epoch: 9    | Elapsed Time:  16.47 s | Train Loss:  0.0432 | Valid Loss:  0.0031 | Train F1:  0.4515 | Valid F1:  0.5042 | Train Precision:  0.3454 | Valid Precision:  0.4152 |  + \n",
      "Epoch: 10   | Elapsed Time:  14.60 s | Train Loss:  0.0435 | Valid Loss:  0.0031 | Train F1:  0.4517 | Valid F1:  0.5002 | Train Precision:  0.3478 | Valid Precision:  0.4095 | \n",
      "Epoch: 11   | Elapsed Time:  13.18 s | Train Loss:  0.0432 | Valid Loss:  0.0031 | Train F1:  0.4539 | Valid F1:  0.4497 | Train Precision:  0.3484 | Valid Precision:  0.3325 | \n",
      "Epoch: 12   | Elapsed Time:  15.68 s | Train Loss:  0.0431 | Valid Loss:  0.0031 | Train F1:  0.4564 | Valid F1:  0.4868 | Train Precision:  0.3503 | Valid Precision:  0.3906 | \n",
      "Epoch: 13   | Elapsed Time:  14.43 s | Train Loss:  0.0433 | Valid Loss:  0.0031 | Train F1:  0.4507 | Valid F1:  0.4681 | Train Precision:  0.3450 | Valid Precision:  0.3578 | \n",
      "Epoch: 14   | Elapsed Time:  14.77 s | Train Loss:  0.0432 | Valid Loss:  0.0031 | Train F1:  0.4527 | Valid F1:  0.4763 | Train Precision:  0.3475 | Valid Precision:  0.3694 | \n",
      "Epoch: 15   | Elapsed Time:  14.69 s | Train Loss:  0.0431 | Valid Loss:  0.0031 | Train F1:  0.4526 | Valid F1:  0.4834 | Train Precision:  0.3447 | Valid Precision:  0.3826 | \n",
      "Epoch: 16   | Elapsed Time:  15.17 s | Train Loss:  0.0430 | Valid Loss:  0.0031 | Train F1:  0.4477 | Valid F1:  0.4741 | Train Precision:  0.3413 | Valid Precision:  0.3628 | \n",
      "Epoch: 17   | Elapsed Time:  15.32 s | Train Loss:  0.0430 | Valid Loss:  0.0031 | Train F1:  0.4480 | Valid F1:  0.4524 | Train Precision:  0.3398 | Valid Precision:  0.3359 | \n",
      "Epoch: 18   | Elapsed Time:  15.13 s | Train Loss:  0.0430 | Valid Loss:  0.0031 | Train F1:  0.4497 | Valid F1:  0.4591 | Train Precision:  0.3432 | Valid Precision:  0.3455 | \n",
      "Epoch: 19   | Elapsed Time:  13.99 s | Train Loss:  0.0434 | Valid Loss:  0.0031 | Train F1:  0.4537 | Valid F1:  0.4760 | Train Precision:  0.3475 | Valid Precision:  0.3762 | \n",
      "Epoch: 20   | Elapsed Time:  14.24 s | Train Loss:  0.0430 | Valid Loss:  0.0031 | Train F1:  0.4532 | Valid F1:  0.4787 | Train Precision:  0.3480 | Valid Precision:  0.3679 | \n",
      "Epoch: 21   | Elapsed Time:  13.39 s | Train Loss:  0.0431 | Valid Loss:  0.0031 | Train F1:  0.4527 | Valid F1:  0.5074 | Train Precision:  0.3443 | Valid Precision:  0.4234 |  + \n",
      "Epoch: 22   | Elapsed Time:  15.46 s | Train Loss:  0.0430 | Valid Loss:  0.0031 | Train F1:  0.4510 | Valid F1:  0.4797 | Train Precision:  0.3444 | Valid Precision:  0.3752 | \n",
      "Epoch: 23   | Elapsed Time:  14.83 s | Train Loss:  0.0430 | Valid Loss:  0.0031 | Train F1:  0.4499 | Valid F1:  0.4520 | Train Precision:  0.3415 | Valid Precision:  0.3345 | \n",
      "Epoch: 24   | Elapsed Time:  15.40 s | Train Loss:  0.0432 | Valid Loss:  0.0031 | Train F1:  0.4444 | Valid F1:  0.4699 | Train Precision:  0.3362 | Valid Precision:  0.3627 | \n",
      "Epoch: 25   | Elapsed Time:  13.50 s | Train Loss:  0.0430 | Valid Loss:  0.0032 | Train F1:  0.4495 | Valid F1:  0.4879 | Train Precision:  0.3432 | Valid Precision:  0.3901 | \n",
      "Epoch: 26   | Elapsed Time:  15.12 s | Train Loss:  0.0428 | Valid Loss:  0.0031 | Train F1:  0.4465 | Valid F1:  0.4516 | Train Precision:  0.3367 | Valid Precision:  0.3392 | \n",
      "Epoch: 27   | Elapsed Time:  15.12 s | Train Loss:  0.0429 | Valid Loss:  0.0032 | Train F1:  0.4473 | Valid F1:  0.5025 | Train Precision:  0.3395 | Valid Precision:  0.4139 | \n",
      "Epoch: 28   | Elapsed Time:  3.79 s | Train Loss:  0.0429 | Valid Loss:  0.0031 | Train F1:  0.4506 | Valid F1:  0.4795 | Train Precision:  0.3423 | Valid Precision:  0.3736 | \n",
      "Epoch: 29   | Elapsed Time:  3.84 s | Train Loss:  0.0429 | Valid Loss:  0.0031 | Train F1:  0.4503 | Valid F1:  0.4740 | Train Precision:  0.3418 | Valid Precision:  0.3663 | \n",
      "Epoch: 30   | Elapsed Time:  3.91 s | Train Loss:  0.0428 | Valid Loss:  0.0031 | Train F1:  0.4473 | Valid F1:  0.4701 | Train Precision:  0.3392 | Valid Precision:  0.3584 | \n",
      "Epoch: 31   | Elapsed Time:  4.02 s | Train Loss:  0.0428 | Valid Loss:  0.0032 | Train F1:  0.4489 | Valid F1:  0.4684 | Train Precision:  0.3392 | Valid Precision:  0.3594 | \n",
      "Epoch: 32   | Elapsed Time:  3.84 s | Train Loss:  0.0429 | Valid Loss:  0.0031 | Train F1:  0.4507 | Valid F1:  0.4698 | Train Precision:  0.3409 | Valid Precision:  0.3610 | \n",
      "Epoch: 33   | Elapsed Time:  3.79 s | Train Loss:  0.0425 | Valid Loss:  0.0032 | Train F1:  0.4553 | Valid F1:  0.4884 | Train Precision:  0.3450 | Valid Precision:  0.3881 | \n",
      "Epoch: 34   | Elapsed Time:  3.90 s | Train Loss:  0.0428 | Valid Loss:  0.0031 | Train F1:  0.4477 | Valid F1:  0.4790 | Train Precision:  0.3376 | Valid Precision:  0.3724 | \n",
      "Epoch: 35   | Elapsed Time:  3.92 s | Train Loss:  0.0427 | Valid Loss:  0.0032 | Train F1:  0.4454 | Valid F1:  0.5120 | Train Precision:  0.3324 | Valid Precision:  0.4298 |  + \n",
      "Epoch: 36   | Elapsed Time:  3.71 s | Train Loss:  0.0429 | Valid Loss:  0.0031 | Train F1:  0.4483 | Valid F1:  0.4494 | Train Precision:  0.3398 | Valid Precision:  0.3295 | \n",
      "Epoch: 37   | Elapsed Time:  3.82 s | Train Loss:  0.0427 | Valid Loss:  0.0031 | Train F1:  0.4516 | Valid F1:  0.4337 | Train Precision:  0.3418 | Valid Precision:  0.3127 | \n",
      "Epoch: 38   | Elapsed Time:  4.02 s | Train Loss:  0.0427 | Valid Loss:  0.0031 | Train F1:  0.4433 | Valid F1:  0.4824 | Train Precision:  0.3313 | Valid Precision:  0.3802 | \n",
      "Epoch: 39   | Elapsed Time:  3.95 s | Train Loss:  0.0430 | Valid Loss:  0.0031 | Train F1:  0.4446 | Valid F1:  0.4939 | Train Precision:  0.3356 | Valid Precision:  0.4007 | \n",
      "Epoch: 40   | Elapsed Time:  4.03 s | Train Loss:  0.0425 | Valid Loss:  0.0032 | Train F1:  0.4434 | Valid F1:  0.4881 | Train Precision:  0.3341 | Valid Precision:  0.3863 | \n",
      "Epoch: 41   | Elapsed Time:  4.08 s | Train Loss:  0.0425 | Valid Loss:  0.0031 | Train F1:  0.4507 | Valid F1:  0.4866 | Train Precision:  0.3396 | Valid Precision:  0.3866 | \n",
      "Epoch: 42   | Elapsed Time:  3.77 s | Train Loss:  0.0427 | Valid Loss:  0.0031 | Train F1:  0.4511 | Valid F1:  0.4440 | Train Precision:  0.3415 | Valid Precision:  0.3275 | \n",
      "Epoch: 43   | Elapsed Time:  3.93 s | Train Loss:  0.0429 | Valid Loss:  0.0032 | Train F1:  0.4454 | Valid F1:  0.5037 | Train Precision:  0.3339 | Valid Precision:  0.4133 | \n",
      "Epoch: 44   | Elapsed Time:  4.03 s | Train Loss:  0.0427 | Valid Loss:  0.0032 | Train F1:  0.4530 | Valid F1:  0.4772 | Train Precision:  0.3442 | Valid Precision:  0.3715 | \n",
      "Epoch: 45   | Elapsed Time:  3.78 s | Train Loss:  0.0426 | Valid Loss:  0.0031 | Train F1:  0.4452 | Valid F1:  0.4736 | Train Precision:  0.3358 | Valid Precision:  0.3691 | \n",
      "Epoch: 46   | Elapsed Time:  4.07 s | Train Loss:  0.0423 | Valid Loss:  0.0032 | Train F1:  0.4508 | Valid F1:  0.4558 | Train Precision:  0.3414 | Valid Precision:  0.3432 | \n",
      "Epoch: 47   | Elapsed Time:  4.09 s | Train Loss:  0.0427 | Valid Loss:  0.0032 | Train F1:  0.4471 | Valid F1:  0.4947 | Train Precision:  0.3348 | Valid Precision:  0.3997 | \n",
      "Epoch: 48   | Elapsed Time:  3.79 s | Train Loss:  0.0426 | Valid Loss:  0.0032 | Train F1:  0.4515 | Valid F1:  0.4620 | Train Precision:  0.3414 | Valid Precision:  0.3525 | \n",
      "Epoch: 49   | Elapsed Time:  3.71 s | Train Loss:  0.0425 | Valid Loss:  0.0032 | Train F1:  0.4512 | Valid F1:  0.4830 | Train Precision:  0.3401 | Valid Precision:  0.3799 | \n",
      "Epoch: 50   | Elapsed Time:  4.00 s | Train Loss:  0.0427 | Valid Loss:  0.0032 | Train F1:  0.4496 | Valid F1:  0.4738 | Train Precision:  0.3390 | Valid Precision:  0.3718 | \n",
      "Epoch: 51   | Elapsed Time:  7.38 s | Train Loss:  0.0424 | Valid Loss:  0.0032 | Train F1:  0.4495 | Valid F1:  0.4979 | Train Precision:  0.3368 | Valid Precision:  0.4121 | \n",
      "Epoch: 52   | Elapsed Time:  10.42 s | Train Loss:  0.0424 | Valid Loss:  0.0032 | Train F1:  0.4492 | Valid F1:  0.4953 | Train Precision:  0.3368 | Valid Precision:  0.4025 | \n",
      "Epoch: 53   | Elapsed Time:  12.00 s | Train Loss:  0.0424 | Valid Loss:  0.0032 | Train F1:  0.4471 | Valid F1:  0.4977 | Train Precision:  0.3354 | Valid Precision:  0.4053 | \n",
      "Epoch: 54   | Elapsed Time:  16.43 s | Train Loss:  0.0425 | Valid Loss:  0.0032 | Train F1:  0.4489 | Valid F1:  0.4348 | Train Precision:  0.3372 | Valid Precision:  0.3187 | \n",
      "Epoch: 55   | Elapsed Time:  15.97 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4446 | Valid F1:  0.4795 | Train Precision:  0.3317 | Valid Precision:  0.3817 | \n",
      "Epoch: 56   | Elapsed Time:  4.17 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4493 | Valid F1:  0.4632 | Train Precision:  0.3349 | Valid Precision:  0.3595 | \n",
      "Epoch: 57   | Elapsed Time:  3.86 s | Train Loss:  0.0423 | Valid Loss:  0.0033 | Train F1:  0.4458 | Valid F1:  0.5179 | Train Precision:  0.3321 | Valid Precision:  0.4484 |  + \n",
      "Epoch: 58   | Elapsed Time:  3.91 s | Train Loss:  0.0423 | Valid Loss:  0.0032 | Train F1:  0.4501 | Valid F1:  0.4733 | Train Precision:  0.3368 | Valid Precision:  0.3681 | \n",
      "Epoch: 59   | Elapsed Time:  3.78 s | Train Loss:  0.0421 | Valid Loss:  0.0032 | Train F1:  0.4512 | Valid F1:  0.4795 | Train Precision:  0.3374 | Valid Precision:  0.3795 | \n",
      "Epoch: 60   | Elapsed Time:  3.91 s | Train Loss:  0.0423 | Valid Loss:  0.0032 | Train F1:  0.4456 | Valid F1:  0.4336 | Train Precision:  0.3317 | Valid Precision:  0.3174 | \n",
      "Epoch: 61   | Elapsed Time:  3.94 s | Train Loss:  0.0424 | Valid Loss:  0.0031 | Train F1:  0.4453 | Valid F1:  0.4335 | Train Precision:  0.3320 | Valid Precision:  0.3127 | \n",
      "Epoch: 62   | Elapsed Time:  3.92 s | Train Loss:  0.0420 | Valid Loss:  0.0032 | Train F1:  0.4408 | Valid F1:  0.4888 | Train Precision:  0.3259 | Valid Precision:  0.3961 | \n",
      "Epoch: 63   | Elapsed Time:  3.81 s | Train Loss:  0.0426 | Valid Loss:  0.0032 | Train F1:  0.4480 | Valid F1:  0.4817 | Train Precision:  0.3358 | Valid Precision:  0.3811 | \n",
      "Epoch: 64   | Elapsed Time:  3.88 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4476 | Valid F1:  0.4481 | Train Precision:  0.3350 | Valid Precision:  0.3330 | \n",
      "Epoch: 65   | Elapsed Time:  3.83 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4510 | Valid F1:  0.4766 | Train Precision:  0.3366 | Valid Precision:  0.3769 | \n",
      "Epoch: 66   | Elapsed Time:  3.88 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4500 | Valid F1:  0.4567 | Train Precision:  0.3372 | Valid Precision:  0.3453 | \n",
      "Epoch: 67   | Elapsed Time:  3.80 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4445 | Valid F1:  0.4445 | Train Precision:  0.3310 | Valid Precision:  0.3283 | \n",
      "Epoch: 68   | Elapsed Time:  3.95 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4546 | Valid F1:  0.4264 | Train Precision:  0.3418 | Valid Precision:  0.3060 | \n",
      "Epoch: 69   | Elapsed Time:  3.79 s | Train Loss:  0.0419 | Valid Loss:  0.0032 | Train F1:  0.4478 | Valid F1:  0.4261 | Train Precision:  0.3329 | Valid Precision:  0.3069 | \n",
      "Epoch: 70   | Elapsed Time:  3.82 s | Train Loss:  0.0423 | Valid Loss:  0.0032 | Train F1:  0.4450 | Valid F1:  0.4639 | Train Precision:  0.3317 | Valid Precision:  0.3535 | \n",
      "Epoch: 71   | Elapsed Time:  3.81 s | Train Loss:  0.0420 | Valid Loss:  0.0032 | Train F1:  0.4470 | Valid F1:  0.4446 | Train Precision:  0.3327 | Valid Precision:  0.3321 | \n",
      "Epoch: 72   | Elapsed Time:  3.65 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4434 | Valid F1:  0.4681 | Train Precision:  0.3293 | Valid Precision:  0.3602 | \n",
      "Epoch: 73   | Elapsed Time:  3.80 s | Train Loss:  0.0418 | Valid Loss:  0.0032 | Train F1:  0.4465 | Valid F1:  0.4382 | Train Precision:  0.3307 | Valid Precision:  0.3207 | \n",
      "Epoch: 74   | Elapsed Time:  3.78 s | Train Loss:  0.0422 | Valid Loss:  0.0032 | Train F1:  0.4475 | Valid F1:  0.4347 | Train Precision:  0.3323 | Valid Precision:  0.3200 | \n",
      "Epoch: 75   | Elapsed Time:  3.83 s | Train Loss:  0.0419 | Valid Loss:  0.0032 | Train F1:  0.4483 | Valid F1:  0.4882 | Train Precision:  0.3314 | Valid Precision:  0.3920 | \n",
      "Epoch: 76   | Elapsed Time:  3.67 s | Train Loss:  0.0421 | Valid Loss:  0.0032 | Train F1:  0.4467 | Valid F1:  0.4692 | Train Precision:  0.3296 | Valid Precision:  0.3609 | \n",
      "Epoch: 77   | Elapsed Time:  3.79 s | Train Loss:  0.0420 | Valid Loss:  0.0032 | Train F1:  0.4458 | Valid F1:  0.4752 | Train Precision:  0.3309 | Valid Precision:  0.3708 | \n",
      "Epoch: 78   | Elapsed Time:  3.88 s | Train Loss:  0.0418 | Valid Loss:  0.0032 | Train F1:  0.4427 | Valid F1:  0.4604 | Train Precision:  0.3261 | Valid Precision:  0.3486 | \n",
      "Epoch: 79   | Elapsed Time:  3.86 s | Train Loss:  0.0419 | Valid Loss:  0.0032 | Train F1:  0.4464 | Valid F1:  0.4436 | Train Precision:  0.3318 | Valid Precision:  0.3237 | \n",
      "Epoch: 80   | Elapsed Time:  4.00 s | Train Loss:  0.0418 | Valid Loss:  0.0032 | Train F1:  0.4513 | Valid F1:  0.4380 | Train Precision:  0.3361 | Valid Precision:  0.3190 | \n",
      "Epoch: 81   | Elapsed Time:  3.94 s | Train Loss:  0.0417 | Valid Loss:  0.0033 | Train F1:  0.4492 | Valid F1:  0.4813 | Train Precision:  0.3320 | Valid Precision:  0.3810 | \n",
      "Epoch: 82   | Elapsed Time:  4.07 s | Train Loss:  0.0417 | Valid Loss:  0.0033 | Train F1:  0.4440 | Valid F1:  0.4837 | Train Precision:  0.3283 | Valid Precision:  0.3926 | \n",
      "Epoch: 83   | Elapsed Time:  3.78 s | Train Loss:  0.0419 | Valid Loss:  0.0032 | Train F1:  0.4432 | Valid F1:  0.4762 | Train Precision:  0.3267 | Valid Precision:  0.3719 | \n",
      "Epoch: 84   | Elapsed Time:  3.92 s | Train Loss:  0.0420 | Valid Loss:  0.0032 | Train F1:  0.4459 | Valid F1:  0.4393 | Train Precision:  0.3308 | Valid Precision:  0.3197 | \n",
      "Epoch: 85   | Elapsed Time:  3.90 s | Train Loss:  0.0418 | Valid Loss:  0.0032 | Train F1:  0.4457 | Valid F1:  0.4434 | Train Precision:  0.3285 | Valid Precision:  0.3287 | \n",
      "Epoch: 86   | Elapsed Time:  3.90 s | Train Loss:  0.0418 | Valid Loss:  0.0032 | Train F1:  0.4436 | Valid F1:  0.4455 | Train Precision:  0.3274 | Valid Precision:  0.3326 | \n",
      "Epoch: 87   | Elapsed Time:  3.90 s | Train Loss:  0.0419 | Valid Loss:  0.0032 | Train F1:  0.4434 | Valid F1:  0.4830 | Train Precision:  0.3274 | Valid Precision:  0.3847 | \n",
      "Epoch: 88   | Elapsed Time:  3.84 s | Train Loss:  0.0417 | Valid Loss:  0.0032 | Train F1:  0.4439 | Valid F1:  0.4739 | Train Precision:  0.3274 | Valid Precision:  0.3685 | \n",
      "Epoch: 89   | Elapsed Time:  3.78 s | Train Loss:  0.0419 | Valid Loss:  0.0032 | Train F1:  0.4490 | Valid F1:  0.4556 | Train Precision:  0.3326 | Valid Precision:  0.3468 | \n",
      "Epoch: 90   | Elapsed Time:  3.98 s | Train Loss:  0.0417 | Valid Loss:  0.0032 | Train F1:  0.4471 | Valid F1:  0.4511 | Train Precision:  0.3305 | Valid Precision:  0.3368 | \n",
      "Epoch: 91   | Elapsed Time:  4.01 s | Train Loss:  0.0418 | Valid Loss:  0.0032 | Train F1:  0.4434 | Valid F1:  0.4347 | Train Precision:  0.3259 | Valid Precision:  0.3166 | \n",
      "Epoch: 92   | Elapsed Time:  3.83 s | Train Loss:  0.0418 | Valid Loss:  0.0033 | Train F1:  0.4446 | Valid F1:  0.4809 | Train Precision:  0.3276 | Valid Precision:  0.3830 | \n",
      "Epoch: 93   | Elapsed Time:  3.77 s | Train Loss:  0.0416 | Valid Loss:  0.0033 | Train F1:  0.4473 | Valid F1:  0.4801 | Train Precision:  0.3295 | Valid Precision:  0.3817 | \n",
      "Epoch: 94   | Elapsed Time:  3.81 s | Train Loss:  0.0416 | Valid Loss:  0.0032 | Train F1:  0.4440 | Valid F1:  0.4714 | Train Precision:  0.3268 | Valid Precision:  0.3684 | \n",
      "Epoch: 95   | Elapsed Time:  3.91 s | Train Loss:  0.0418 | Valid Loss:  0.0032 | Train F1:  0.4509 | Valid F1:  0.4670 | Train Precision:  0.3342 | Valid Precision:  0.3612 | \n",
      "Epoch: 96   | Elapsed Time:  4.17 s | Train Loss:  0.0417 | Valid Loss:  0.0032 | Train F1:  0.4484 | Valid F1:  0.4608 | Train Precision:  0.3329 | Valid Precision:  0.3563 | \n",
      "Epoch: 97   | Elapsed Time:  4.20 s | Train Loss:  0.0415 | Valid Loss:  0.0032 | Train F1:  0.4486 | Valid F1:  0.4760 | Train Precision:  0.3319 | Valid Precision:  0.3748 | \n",
      "Epoch: 98   | Elapsed Time:  3.94 s | Train Loss:  0.0417 | Valid Loss:  0.0032 | Train F1:  0.4465 | Valid F1:  0.4451 | Train Precision:  0.3304 | Valid Precision:  0.3296 | \n",
      "Epoch: 99   | Elapsed Time:  3.98 s | Train Loss:  0.0419 | Valid Loss:  0.0032 | Train F1:  0.4438 | Valid F1:  0.4611 | Train Precision:  0.3272 | Valid Precision:  0.3517 | \n",
      "Total Time elapsed:  713.9504\n",
      "======================== End of Training ===================\n",
      " *********************** SUMMARY FOR VALIDATION ***********************\n",
      "  Best Model loss: 0.003279519636923969\n",
      "  Best Model F1 Score: 0.5179211469534051\n",
      "  Best Model Precision: 0.4484096198603569\n",
      " *********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from bank_telemarketing.train.engine import engine\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    engine(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=cyclic_optimizer,\n",
    "        train_dataloader=train_dataloader,\n",
    "        eval_dataloader=test_dataloader,\n",
    "        config=config,\n",
    "        scheduler=cyclic_scheduler,\n",
    "        pos_weight=pos_weight,\n",
    "        logs_dir=\"../logs\",\n",
    "        checkpoint_dir=\"../models_storage\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint = torch.load(os.path.join(\"../models_storage\", \"checkpoint-04484096198603569.pt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Classifier(\n  (dropout): Dropout(p=0.3, inplace=False)\n  (bn): BatchNorm1d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (embedding_layers): ModuleList(\n    (0): Embedding(13, 7)\n    (1): Embedding(5, 3)\n    (2): Embedding(9, 5)\n    (3): Embedding(4, 2)\n    (4): Embedding(4, 2)\n    (5): Embedding(4, 2)\n    (6): Embedding(11, 6)\n    (7): Embedding(6, 3)\n    (8): Embedding(4, 2)\n  )\n  (linear): Sequential(\n    (0): Linear(in_features=43, out_features=64, bias=True)\n    (1): ReLU()\n    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): ReLU()\n    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Linear(in_features=32, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = checkpoint[\"best_state_dict\"]\n",
    "m = Classifier(\n",
    "    hidden=64,\n",
    "    output_size=1,\n",
    "    continuous_size=11,\n",
    "    embedding_sizes=emb,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "m.load_state_dict(state_dict)\n",
    "m.to(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def predict(model, x_cont, x_emb, cut_point: float, device: str = \"cpu\") -> (List[float], List[float]):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_emb = x_emb.to(device)\n",
    "        x_cont = x_cont.to(device)\n",
    "\n",
    "        logits = model(x_cont, x_emb).squeeze(1)\n",
    "\n",
    "        prob = torch.sigmoid(logits).detach().tolist()\n",
    "        y_pred = [p >= cut_point for p in prob]\n",
    "    return prob, y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=200\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17705123126506805, 0.1394490897655487, 0.5402640104293823]\n",
      "Prob: [0.17705123126506805, 0.1394490897655487, 0.5402640104293823]\n",
      "Prediction: [False, False, True]\n",
      "True label: tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    x_cont = batch[\"x_cont\"]\n",
    "    x_emb = batch[\"x_emb\"]\n",
    "    y = batch[\"y\"]\n",
    "\n",
    "    prob, pred = predict(\n",
    "        model=m,\n",
    "        x_cont=x_cont,\n",
    "        x_emb=x_emb,\n",
    "        cut_point=0.5,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    print(f\"Prob: {prob}\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"True label: {y}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bank_telemarketing.utils import model_performance\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, pos_weight, cut_point=0.5):\n",
    "    y_trues = []\n",
    "    losses = []\n",
    "    probs = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_emb = batch[\"x_emb\"].to(device)\n",
    "            x_cont = batch[\"x_cont\"].to(device)\n",
    "            y_true = batch[\"y\"].to(device)\n",
    "\n",
    "            logits = model(x_cont, x_emb)\n",
    "            logits = logits.squeeze(1)\n",
    "\n",
    "            loss = criterion(logits, y_true.float())\n",
    "\n",
    "            losses.append(loss.detach().cpu().item() * len(batch))\n",
    "            probs.extend(torch.sigmoid(logits).detach().tolist())\n",
    "            y_trues.extend(y_true.detach().cpu().tolist())\n",
    "\n",
    "        y_preds = [p >= cut_point for p in probs]\n",
    "        acc = accuracy_score(y_pred=y_preds, y_true=y_trues)\n",
    "        # print(len(y_true))\n",
    "        # print(len(y_preds))\n",
    "\n",
    "        train_performance = model_performance(\n",
    "            y_true=y_trues,\n",
    "            y_pred=y_preds,\n",
    "            losses=losses,\n",
    "            pos_weight=pos_weight,\n",
    "        )\n",
    "        return train_performance, acc, y_preds, y_trues"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "perf = evaluate(\n",
    "    model=m,\n",
    "    dataloader=test_dataloader,\n",
    "    device=\"cuda\",\n",
    "    cut_point=0.5,\n",
    "    pos_weight=pos_weight\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "({'loss': 0.015003011115492404,\n  'f1': 0.5179211469534051,\n  'precision': 0.4484096198603569},\n 0.8693540553666829,\n [False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  True,\n  True,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  True,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  True,\n  True,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  True,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  False,\n  True,\n  ...],\n [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  1,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  0,\n  1,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  1,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  1,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  1,\n  0,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  ...])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(y_true=perf[3], y_pred=perf[2])\n",
    "sns.heatmap(cm / np.sum(cm), annot=True, fmt=\".2%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}